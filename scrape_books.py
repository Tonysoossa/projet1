import os
import requests
from bs4 import BeautifulSoup
import csv
import json
import time
from urllib.parse import urljoin
import scrape_book_details as sbd

def get_all_categories():
    """
    R√©cup√®re toutes les cat√©gories disponibles sur le site books.toscrape.com
    """
    base_url = "https://books.toscrape.com/"
    response = requests.get(base_url)
    soup = BeautifulSoup(response.text, 'lxml')

    categories = {}

    category_links = soup.select('div.side_categories ul li ul li a')

    for link in category_links:
        category_name = link.get_text(strip=True)
        category_url = urljoin(base_url, str(link['href']))
        categories[category_name] = category_url

    return categories

def scrape_category_books(category_name, category_url, global_book_counter):
    """
    Scrape tous les livres d'une cat√©gorie donn√©e
    """
    print(f"\nScraping de la cat√©gorie: {category_name}")
    print(f"URL: {category_url}")

    all_books_data = []
    page = 1

    while True:
        if page == 1:
            url = category_url
        else:
            url = category_url.replace('index.html', f'page-{page}.html')

        print(f"Page {page}...")
        response = requests.get(url)

        if response.status_code == 404:
            print(f"Fin de la cat√©gorie {category_name} (page {page-1})")
            continue

        soup = BeautifulSoup(response.text, 'lxml')
        books = soup.select('.product_pod')

        if not books:
            print(f"Fin de la cat√©gorie {category_name} (aucun livre sur la page {page})")
            continue

        for book in books:
            try:
                link = book.select('h3 > a')[0]['href']
                book_url = urljoin(url, str(link))

                book_data = sbd.scrape_book_details(book_url, global_book_counter[0])
                book_data['number'] = global_book_counter[0]
                book_data['category'] = category_name  # Forcer la cat√©gorie

                all_books_data.append(book_data)
                print(f"[{global_book_counter[0]:03d}] {book_data['title']}")
                global_book_counter[0] += 1

                time.sleep(0.2)

            except Exception as e:
                print(f"‚ùå Erreur lors du scraping d'un livre: {e}")
                continue

        page += 1

    return all_books_data

def scrape_all_categories():
    """
    Fonction principale qui scrape toutes les cat√©gories
    """
    if not os.path.exists('images'):
        os.makedirs('images')
        print("üìÅ Dossier 'images' cr√©√©")

    print("üìã R√©cup√©ration de la liste des cat√©gories...")
    categories = get_all_categories()

    if not categories:
        print("‚ùå Aucune cat√©gorie trouv√©e!")
        return

    print(f"üìä {len(categories)} cat√©gories trouv√©es:")
    for i, category_name in enumerate(categories.keys(), 1):
        print(f"  {i:2d}. {category_name}")

    all_books_data = []
    global_book_counter = [1]

    for i, (category_name, category_url) in enumerate(categories.items(), 1):
        print(f"\n{'='*60}")
        print(f"Cat√©gorie {i}/{len(categories)}: {category_name}")
        print(f"{'='*60}")

        try:
            category_books = scrape_category_books(category_name, category_url, global_book_counter)
            all_books_data.extend(category_books)

            print(f"Cat√©gorie '{category_name}' termin√©e: {len(category_books)} livres")

        except Exception as e:
            print(f"‚ùå Erreur lors du scraping de la cat√©gorie '{category_name}': {e}")
            continue

        time.sleep(0.5)

    save_data(all_books_data)

    print("\n SCRAPING TERMIN√â!")
    print(f"Total: {len(all_books_data)} livres r√©cup√©r√©s")

def save_data(all_books_data):
    """
    Sauvegarde les donn√©es dans des fichiers CSV et JSON
    """
    if not all_books_data:
        print("Aucune donn√©e √† sauvegarder")
        return

    fieldnames = ['number', 'upc', 'title', 'price_including_tax', 'price_excluding_tax',
                  'number_available', 'product_description', 'category', 'review_rating',
                  'image_url', 'book_url', 'local_image_path']

    print("üíæ Sauvegarde en CSV...")
    with open('all_books.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_books_data)

    print("üíæ Sauvegarde en JSON...")
    with open('all_books.json', 'w', encoding='utf-8') as f:
        json.dump(all_books_data, f, ensure_ascii=False, indent=2)

    print("Donn√©es sauvegard√©es dans 'all_books.csv' et 'all_books.json'")
